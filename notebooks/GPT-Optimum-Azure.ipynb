{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-toronto",
   "metadata": {},
   "source": [
    "# Pretrained  GPT2  Optimum Model Deployment Example\n",
    "\n",
    "In this notebook, we will run an example of text generation using GPT2 model exported from HuggingFace and deployed with Seldon's MLServer pre-packed server using Optimim library as a runtime.\n",
    "\n",
    "After we have the module deployed to Kubernetes, we will run a simple load test to evaluate the module inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "assigned-diesel",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/lenisha/.local/lib/python3.8/site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/lenisha/.local/lib/python3.8/site-packages (from requests) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ec7df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parallel_workers\": 0,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-generation\",\n",
    "            \"pretrained_model\": \"distilgpt2\"\n",
    "             \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50ddda25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-17 00:55:44,032 [mlserver] INFO - Using asyncio event-loop policy: uvloop\n",
      "2022-05-17 00:55:44.904153: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-17 00:55:44.904379: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
      "ImportError: numpy.core.multiarray failed to import\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "SystemError: <built-in function __import__> returned a result with an error set\n",
      "2022-05-17 00:55:46.518842: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-17 00:55:46.519081: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-17 00:55:46.519092: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-17 00:55:46.519110: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-UV3QRV5): /proc/driver/nvidia/version does not exist\n",
      "2022-05-17 00:55:46.519479: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-17 00:55:46.520634: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-17 00:55:46,848 [mlserver] WARNING - Model name 'transformer' is different than model's folder name '.'.\n",
      "2022-05-17 00:55:46,942 [mlserver.parallel] DEBUG - Starting response processing loop...\n",
      "INFO:     Started server process [14624]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "2022-05-17 00:55:46,988 [mlserver.grpc] INFO - gRPC server running on http://0.0.0.0:8081\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "2022-05-17 00:55:56,548 [mlserver] INFO - Loaded model 'transformer' succesfully.\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "process = Popen(\n",
    "     [\"mlserver\",\"start\", \".\"],\n",
    "    stdout=PIPE,\n",
    "    stderr=STDOUT,\n",
    "    close_fds=True,\n",
    ")\n",
    "for line in iter(process.stdout.readline, b''):\n",
    "    linestr = line.rstrip().decode('utf-8')\n",
    "    print(linestr)\n",
    "    if linestr.find(\"Loaded model\")!=-1:\n",
    "       break\n",
    "process.stdout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "lasting-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'transformer', 'model_version': None, 'id': '0e7db5bb-dc5e-4478-aa8a-9813f7df6dc4', 'parameters': None, 'outputs': [{'name': 'output', 'shape': [1], 'datatype': 'BYTES', 'parameters': None, 'data': ['[{\"generated_text\": \"I love Artificial Intelligence. But we have yet to see one of these Artificial Intelligence products, and we think we\\'re probably never going to see it as a product we\\'re considering.\\\\n\\\\n\\\\n\\\\\"It\\'s not even been announced yet,\\\\\" a team\"}]']}]}\n",
      "Elapsed time: 0.8587948680506088\n"
     ]
    }
   ],
   "source": [
    "# Copy model file\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"args\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"I love Artificial Intelligence\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "import time\n",
    "start_time = time.monotonic()\n",
    "\n",
    "response = requests.post(\"http://localhost:8080/v2/models/transformer/infer\", json=inference_request).json() \n",
    "print(response)\n",
    "print(f\"Elapsed time: {time.monotonic() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47bbffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "process.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0e1d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8912c066",
   "metadata": {},
   "source": [
    "#Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ec7df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parallel_workers\": 0,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-classification\"\n",
    "          \n",
    "           \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50ddda25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-17 01:00:23,798 [mlserver] INFO - Using asyncio event-loop policy: uvloop\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lenisha/.local/bin/mlserver\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/lenisha/.local/lib/python3.8/site-packages/mlserver/cli/main.py\", line 78, in main\n",
      "    root()\n",
      "  File \"/usr/lib/python3/dist-packages/click/core.py\", line 764, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/click/core.py\", line 717, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/usr/lib/python3/dist-packages/click/core.py\", line 1137, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/usr/lib/python3/dist-packages/click/core.py\", line 956, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/usr/lib/python3/dist-packages/click/core.py\", line 555, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/home/lenisha/.local/lib/python3.8/site-packages/mlserver/cli/main.py\", line 20, in wrapper\n",
      "    return asyncio.run(f(*args, **kwargs))\n",
      "  File \"/usr/lib/python3.8/asyncio/runners.py\", line 43, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"uvloop/loop.pyx\", line 1501, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"/home/lenisha/.local/lib/python3.8/site-packages/mlserver/cli/main.py\", line 41, in start\n",
      "    settings, models_settings = await load_settings(folder)\n",
      "  File \"/home/lenisha/.local/lib/python3.8/site-packages/mlserver/cli/serve.py\", line 36, in load_settings\n",
      "    models_settings = await repository.list()\n",
      "  File \"/home/lenisha/.local/lib/python3.8/site-packages/mlserver/repository.py\", line 31, in list\n",
      "    model_settings = self._load_model_settings(model_settings_path)\n",
      "  File \"/home/lenisha/.local/lib/python3.8/site-packages/mlserver/repository.py\", line 44, in _load_model_settings\n",
      "    model_settings = ModelSettings.parse_file(model_settings_path)\n",
      "  File \"pydantic/main.py\", line 546, in pydantic.main.BaseModel.parse_file\n",
      "  File \"pydantic/parse.py\", line 64, in pydantic.parse.load_file\n",
      "  File \"pydantic/parse.py\", line 37, in pydantic.parse.load_str_bytes\n",
      "  File \"/usr/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/usr/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 10 column 9 (char 234)\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "process2 = Popen(\n",
    "     [\"mlserver\",\"start\", \".\"],\n",
    "    stdout=PIPE,\n",
    "    stderr=STDOUT,\n",
    "    close_fds=True,\n",
    ")\n",
    "for line in iter(process2.stdout.readline, b''):\n",
    "    linestr = line.rstrip().decode('utf-8')\n",
    "    print(linestr)\n",
    "    if linestr.find(\"Loaded model\")!=-1:\n",
    "       break\n",
    "process2.stdout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "lasting-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'transformer', 'model_version': None, 'id': 'a1978495-5be2-4411-a746-0e7615532c4a', 'parameters': None, 'outputs': [{'name': 'output', 'shape': [1], 'datatype': 'BYTES', 'parameters': None, 'data': ['[{\"label\": \"LABEL_0\", \"score\": 0.9389998912811279}]']}]}\n",
      "Elapsed time: 0.03840390901314095\n"
     ]
    }
   ],
   "source": [
    "# Copy model file\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"args\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"This is awesome!\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "import time\n",
    "start_time = time.monotonic()\n",
    "\n",
    "response = requests.post(\"http://localhost:8080/v2/models/transformer/infer\", json=inference_request).json() \n",
    "print(response)\n",
    "print(f\"Elapsed time: {time.monotonic() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47bbffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "process2.kill()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
