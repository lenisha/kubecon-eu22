{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-toronto",
   "metadata": {},
   "source": [
    "# Pretrained  GPT2  Optimum Model Deployment Example\n",
    "\n",
    "In this notebook, we will run an example of text generation using GPT2 model exported from HuggingFace and deployed with Seldon's MLServer pre-packed server using Optimim library as a runtime.\n",
    "\n",
    "After we have the module deployed to Kubernetes, we will run a simple load test to evaluate the module inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "assigned-diesel",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/lenisha/.local/lib/python3.8/site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/lenisha/.local/lib/python3.8/site-packages (from requests) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ec7df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parallel_workers\": 0,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-generation\",\n",
    "            \"pretrained_model\": \"distilgpt2\"\n",
    "             \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50ddda25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-17 00:55:44,032 [mlserver] INFO - Using asyncio event-loop policy: uvloop\n",
      "2022-05-17 00:55:44.904153: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-17 00:55:44.904379: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
      "ImportError: numpy.core.multiarray failed to import\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "SystemError: <built-in function __import__> returned a result with an error set\n",
      "2022-05-17 00:55:46.518842: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-17 00:55:46.519081: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-17 00:55:46.519092: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-17 00:55:46.519110: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-UV3QRV5): /proc/driver/nvidia/version does not exist\n",
      "2022-05-17 00:55:46.519479: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-17 00:55:46.520634: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-17 00:55:46,848 [mlserver] WARNING - Model name 'transformer' is different than model's folder name '.'.\n",
      "2022-05-17 00:55:46,942 [mlserver.parallel] DEBUG - Starting response processing loop...\n",
      "INFO:     Started server process [14624]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "2022-05-17 00:55:46,988 [mlserver.grpc] INFO - gRPC server running on http://0.0.0.0:8081\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "2022-05-17 00:55:56,548 [mlserver] INFO - Loaded model 'transformer' succesfully.\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "process = Popen(\n",
    "     [\"mlserver\",\"start\", \".\"],\n",
    "    stdout=PIPE,\n",
    "    stderr=STDOUT,\n",
    "    close_fds=True,\n",
    ")\n",
    "for line in iter(process.stdout.readline, b''):\n",
    "    linestr = line.rstrip().decode('utf-8')\n",
    "    print(linestr)\n",
    "    if linestr.find(\"Loaded model\")!=-1:\n",
    "       break\n",
    "process.stdout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "lasting-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'transformer', 'model_version': None, 'id': '0e7db5bb-dc5e-4478-aa8a-9813f7df6dc4', 'parameters': None, 'outputs': [{'name': 'output', 'shape': [1], 'datatype': 'BYTES', 'parameters': None, 'data': ['[{\"generated_text\": \"I love Artificial Intelligence. But we have yet to see one of these Artificial Intelligence products, and we think we\\'re probably never going to see it as a product we\\'re considering.\\\\n\\\\n\\\\n\\\\\"It\\'s not even been announced yet,\\\\\" a team\"}]']}]}\n",
      "Elapsed time: 0.8587948680506088\n"
     ]
    }
   ],
   "source": [
    "# Copy model file\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"args\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"I love Artificial Intelligence\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "import time\n",
    "start_time = time.monotonic()\n",
    "\n",
    "response = requests.post(\"http://localhost:8080/v2/models/transformer/infer\", json=inference_request).json() \n",
    "print(response)\n",
    "print(f\"Elapsed time: {time.monotonic() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47bbffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "process.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0e1d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8912c066",
   "metadata": {},
   "source": [
    "#Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ec7df91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model-settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model-settings.json\n",
    "{\n",
    "    \"name\": \"transformer\",\n",
    "    \"implementation\": \"mlserver_huggingface.HuggingFaceRuntime\",\n",
    "    \"parallel_workers\": 0,\n",
    "    \"parameters\": {\n",
    "        \"extra\": {\n",
    "            \"task\": \"text-classification\"\n",
    "          \n",
    "           \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50ddda25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-17 01:00:45,919 [mlserver] INFO - Using asyncio event-loop policy: uvloop\n",
      "2022-05-17 01:00:47.247707: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-17 01:00:47.247789: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\n",
      "ImportError: numpy.core.multiarray failed to import\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "SystemError: <built-in function __import__> returned a result with an error set\n",
      "2022-05-17 01:00:51.163435: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-17 01:00:51.164054: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-17 01:00:51.164120: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-17 01:00:51.164163: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-UV3QRV5): /proc/driver/nvidia/version does not exist\n",
      "2022-05-17 01:00:51.164632: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-17 01:00:51.165546: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-17 01:00:51,761 [mlserver] WARNING - Model name 'transformer' is different than model's folder name '.'.\n",
      "2022-05-17 01:00:51,892 [mlserver.parallel] DEBUG - Starting response processing loop...\n",
      "INFO:     Started server process [15217]\n",
      "INFO:     Waiting for application startup.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "INFO:     Application startup complete.\n",
      "2022-05-17 01:00:52,142 [mlserver.grpc] INFO - gRPC server running on http://0.0.0.0:8081\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n",
      "Downloading: 100%|██████████| 629/629 [00:00<00:00, 519kB/s]\n",
      "Downloading: 100%|██████████| 255M/255M [00:25<00:00, 10.3MB/s] \n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 28.5kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 1.81MB/s]\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "2022-05-17 01:01:24,623 [mlserver] INFO - Loaded model 'transformer' succesfully.\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "process2 = Popen(\n",
    "     [\"mlserver\",\"start\", \".\"],\n",
    "    stdout=PIPE,\n",
    "    stderr=STDOUT,\n",
    "    close_fds=True,\n",
    ")\n",
    "for line in iter(process2.stdout.readline, b''):\n",
    "    linestr = line.rstrip().decode('utf-8')\n",
    "    print(linestr)\n",
    "    if linestr.find(\"Loaded model\")!=-1:\n",
    "       break\n",
    "process2.stdout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "lasting-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'transformer', 'model_version': None, 'id': 'fc8d9df6-eeb7-4131-b72c-fc77c8ca4658', 'parameters': None, 'outputs': [{'name': 'output', 'shape': [1], 'datatype': 'BYTES', 'parameters': None, 'data': ['[{\"label\": \"POSITIVE\", \"score\": 0.9998669624328613}]']}]}\n",
      "Elapsed time: 0.05480770202120766\n"
     ]
    }
   ],
   "source": [
    "# Copy model file\n",
    "inference_request = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "          \"name\": \"args\",\n",
    "          \"shape\": [1],\n",
    "          \"datatype\": \"BYTES\",\n",
    "          \"data\": [\"This is awesome!\"],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "import time\n",
    "start_time = time.monotonic()\n",
    "\n",
    "response = requests.post(\"http://localhost:8080/v2/models/transformer/infer\", json=inference_request).json() \n",
    "print(response)\n",
    "print(f\"Elapsed time: {time.monotonic() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47bbffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "process2.kill() "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
